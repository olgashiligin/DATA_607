---
title: "Project 4"
author: "Olga Shiligin"
date: "04/11/2018"
output: html_document
---

Project 4 - Text Mining
-------------------------

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:  https://spamassassin.apache.org/publiccorpus/


```{r}
library(tm)
library(RTextTools)
library(SparseM)
```

Making paths to all folders

```{r}
resource_dir = "/Users/Olga/Documents/R/"
ham_train_dir = paste0(resource_dir,"train/easy_ham/")
spam_test_dir = paste0(resource_dir,"test/spam/")
spam_train_dir = paste0(resource_dir,"train/spam_2/")
ham_test_dir = paste0(resource_dir,"test/easy_ham_2/")
```

Ð¡reating corpus for each record type

```{r}
spam_train <- VCorpus(DirSource(spam_train_dir)) 
ham_train <- VCorpus(DirSource(ham_train_dir))
spam_test <- VCorpus(DirSource(spam_test_dir))
ham_test <- VCorpus(DirSource(ham_test_dir))
```

Union all corpuses into one

```{r}
corpus <- c(spam_train, ham_train,spam_test,ham_test)
```
Preparing data: incoding issues, remove punctuation, conver to lower case.

```{r}
corpus <- tm_map(corpus, content_transformer(function(x) iconv(x, "us-ascii", sub="byte")))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
```
 
Sticking the data into a matrix

```{r}
dtm_email <- DocumentTermMatrix(corpus)
```

Remove elements with insignificant effect on results

```{r}
dtm_email <- removeSparseTerms(dtm_email, 0.9)
```
 Labeling the data 

```{r}
email_labels <- unlist(c(
                        rep(list("ham"), length(ham_train)),
                        rep(list("spam"), length(spam_train)),
                        rep(list("ham"), length(ham_test)),
                        rep(list("spam"), length(spam_test))
                        )
                       )
```

Calculate some constant values

```{r}
train_length <- length(spam_train) + length(ham_train)
test_length <- length(ham_test) + length(spam_test)
dataset_size <- length(corpus)
```


Creating cottainer for analysis

```{r}
email_container <- create_container(dtm_email,
                                    labels = email_labels,
                                    trainSize = 1:train_length,
                                    testSize = (train_length+1):dataset_size,
                                    virgin = F)

```

 Lets do try all train model algorithms RTextTools has to offer:"SVM","SLDA","BOOSTING","BAGGING","RF","GLMNET","TREE","MAXENT". I'm not going to use NNET as it is requires a slightly different approach
 
```{r}
svm_model <- train_model(email_container, "SVM")
slda_model <- train_model(email_container, "SLDA")
boosting_model <- train_model(email_container, "BOOSTING")
bagging_model <- train_model(email_container, "BAGGING")
rf_model <- train_model(email_container, "RF")
glmnet_model <- train_model(email_container, "GLMNET")
tree_model <- train_model(email_container, "TREE")
maxent_model <- train_model(email_container, "MAXENT")
```
 
 get original labels for test dataset 
 
```{r} 
correct_email_type <- email_labels[(train_length+1):dataset_size]
```

Classify models and get them into a DataFrame

```{r}
classifying <- c(classify_model(email_container, svm_model),
                 classify_model(email_container, slda_model),
                 classify_model(email_container, boosting_model),
                 classify_model(email_container, bagging_model),
                 classify_model(email_container, rf_model),
                 classify_model(email_container, glmnet_model),
                 classify_model(email_container, tree_model),
                 classify_model(email_container, maxent_model),
                 as.data.frame(correct_email_type)
                )

results <- data.frame(classifying)
```

results for SVM model
```{r}
table(results[,1] == results[,17])/1901
```
 results for SLDA model
```{r}
table(results[,3] == results[,17])/1901
```
 results for BOOSTING model
```{r}
table(results[,5] == results[,17])/1901
```
 results for BAGGING model
```{r}
table(results[,7] == results[,17])/1901
```
 results for RF model
```{r}
table(results[,9] == results[,17])/1901
```
 results for GLMNET model
```{r}
table(results[,11] == results[,17])/1901
```
 results for TREE model
```{r}
table(results[,13] == results[,17])/1901
```
 results for MAXENT model
```{r}
table(results[,15] == results[,17])/1901
```
And the winner is... Bagging algorithm:
